[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///home/trungdq/StreamHandler/src/main/scala/StreamHandler.scala","languageId":"scala","version":1,"text":"import org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.cassandra._\nimport com.datastax.spark.connector._\n\ncase class UserData(id: Int, firstName: String, lastName: String)\n\nobject StreamHandler {\n    def main(args: Array[String]) {\n        val spark = SparkSession\n            .builder\n            .appName(\"Stream Handler\")\n            .config(\"spark.cassandra.connection.host\", \"localhost\")\n            .getOrCreate()\n\n        spark.sparkContext.setLogLevel(\"ERROR\")\n\n        import spark.implicits._\n\n        val inputDF = spark\n            .readStream\n            .format(\"kafka\")\n            .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n            .option(\"subscribe\", \"test_json\")\n            .load()\n\n        inputDF.printSchema()\n\n        val rawDF = inputDF.selectExpr(\"CAST(value AS STRING)\")\n\n        val schema = new StructType()\n            .add(\"id\", IntegerType)\n            .add(\"firstName\", StringType)\n            .add(\"lastName\", StringType)\n\n        val userDF = rawDF.select(from_json(col(\"value\"), schema).as(\"data\"))\n                            .select(\"data.*\")\n\n        val summary = userDF.withColumnRenamed(\"id\", \"id\")\n            .withColumnRenamed(\"firstName\", \"first_name\")\n            .withColumnRenamed(\"lastName\", \"last_name\")\n\n        val query = summary\n            .writeStream\n            .foreachBatch { (batchDF: DataFrame, batchID: Long) =>\n                println(s\"Writing to Cassandra $batchID\")\n                batchDF.write\n                    .cassandraFormat(\"user\", \"test_cas\")\n                    .mode(\"append\")\n                    .save()\n            }\n            .outputMode(\"update\")\n            .start()\n\n        query.awaitTermination()\n    }\n}"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (/home/trungdq/StreamHandler/target/scala-2.12/zinc/inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 0 s, completed Dec 26, 2022 8:33:56 PM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mForcing garbage collection...[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, â™¨1, shutdown, null})[0m
